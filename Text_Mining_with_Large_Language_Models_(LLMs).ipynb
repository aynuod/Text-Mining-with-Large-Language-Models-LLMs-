{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+v7EfDcWL06dWZCNTkyNs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aynuod/Text-Mining-with-Large-Language-Models-LLMs-/blob/main/Text_Mining_with_Large_Language_Models_(LLMs).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Mining with LLMs in Colab\n",
        "\n",
        "### **Objectives**\n",
        "1. To demonstrate how pretrained Large Language Models (LLMs) can simplify text mining tasks.\n",
        "2. To perform sentiment analysis on a sample dataset to classify text as positive or negative.\n",
        "3. To summarize a long paragraph using an encoder-decoder Transformer model.\n",
        "4. To generate creative text using GPT-2, showcasing its generative capabilities.\n",
        "\n",
        "---\n",
        "\n",
        "### **About the Lab**\n",
        "This lab uses the **Hugging Face Transformers library** to access pretrained models. These models were trained on large text datasets and are now fine-tuned for specific tasks like:\n",
        "- Sentiment analysis.\n",
        "- Summarization.\n",
        "- Text generation.\n",
        "\n",
        "By leveraging these models, we save time, avoid the need for expensive training, and focus on using the models for practical applications.\n"
      ],
      "metadata": {
        "id": "9KuR6j33OPrm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rUWAGjFZBzRX"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "\n",
        "# Sample dataset\n",
        "data = {\n",
        "    \"text\": [\n",
        "        \"I love this product! It's fantastic.\",\n",
        "        \"The experience was awful. Totally disappointed.\"\n",
        "    ]\n",
        "}\n",
        "df = pd.DataFrame(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained sentiment analysis model\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Apply sentiment analysis\n",
        "df['sentiment'] = df['text'].apply(lambda x: sentiment_analyzer(x)[0]['label'])\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkfXQD3GCnJS",
        "outputId": "01f318bb-7e3b-4fc4-d68a-d98543933f7f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              text sentiment\n",
            "0             I love this product! It's fantastic.  POSITIVE\n",
            "1  The experience was awful. Totally disappointed.  NEGATIVE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Summarization pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "# Summarize long text\n",
        "text = \"\"\"\n",
        "The Transformer architecture itself is quite impressive. It can be highly\n",
        "parallelized and scaled in ways that previous state of the art NLP models could\n",
        "not be, allowing it to scale to much larger data sets and training times than\n",
        "previous NLP models. The Transformer uses a special kind of attention\n",
        "calculation called self-attention to allow each word in a sequence to “attend to”\n",
        "(look to for context) all other words in the sequence, enabling it to capture longrange\n",
        "dependencies and contextual relationships between words. Of course, no\n",
        "architecture is perfect. Transformers are still limited to an input context window\n",
        "which represents the maximum length of text it can process at any given\n",
        "moment.\n",
        "Since the advent of the Transformer in 2017, the ecosystem around using and\n",
        "deploying Transformers has only exploded. The aptly named “Transformers”\n",
        "library and its supporting packages have made it accessible for practitioners to\n",
        "use, train, and share models, greatly accelerating its adoption and being used by\n",
        "thousands of organizations and counting. Popular LLM repositories like\n",
        "Hugging Face have popped up, providing access to powerful open-source\n",
        "models to the masses. In short, using and productionizing a Transformer has\n",
        "never been easier.\n",
        "\"\"\"\n",
        "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
        "print(summary[0]['summary_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF9qKEbECsz8",
        "outputId": "24875d1a-8207-4694-9845-3f0e41b0d02f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The Transformer architecture itself is quite impressive. It can be highly parallelized and scaled in ways that previous state of the art NLP models could not be, allowing it to scale to much larger data sets and training times than previous models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n",
        "# Text generation pipeline\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\", token=\"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\")\n",
        "\n",
        "# Generate text based on a prompt\n",
        "prompt = \"Artificial intelligence in healthcare is\"\n",
        "output = generator(prompt, max_length=50, num_return_sequences=1)\n",
        "print(output[0]['generated_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YA7CnGSDKtw",
        "outputId": "d2be287a-dcf1-4010-b607-0e0ebf32be53"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `tpllms` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `tpllms`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial intelligence in healthcare is an area that has gained significant interest since the beginning of AI research. There have been several years under way, but what emerges to me is that there has been minimal progress on basic AI technology. We don't need to\n"
          ]
        }
      ]
    }
  ]
}